.. contents:: Table of Contents
      :depth: 6

===============================================
VNI based L2 switching and L3 router forwarding
===============================================

https://git.opendaylight.org/gerrit/#/q/topic:vni-based-l2-l3

**Important: All gerrit links raised for this feature will have topic name as "vni-based-l2-l3"**

This feature realizes VNI (Virtual Network Identifier) based L2 switching and L3 router forwarding.
In doing so, it eliminates the usage of ``LPortTags``, ``ELAN tags`` and ``MPLS labels`` for
realizing the same on the wire. Instead, it makes use of VNIs supplied by the OpenStack using
tenant.
It is a step towards using VNIs in datapath holistically for all switching and routing use-cases.

**Note**: The existing L3 BGPVPN control-path and data-path semantics will be retained to continue
 using ``MPLS Labels`` and ``LPort tags`` to realize both intra-dc and inter-dc connectivity.


Problem description
===================

OpenDaylight NetVirt service today supports VLAN-based, VxLAN-based connectivity and
MPLSOverGRE-based overlays. Amongst these, VxLAN-based underlay is supported only for traffic
within the DataCenter. For all the traffic that needs to go via the DC-Gateway the only supported
underlay is MPLSOverGRE.

Further, MPLS Label based datapath semantics should be used when configured so by the tenant.
Today, even for VxLAN enabled networks by the tenant, the labels are generated by L3 forwarding
service and used. Such labels are re-used for inter-DC use-cases with BGPVPN as well. This does not
honour and is not in accordance with the datapath semantics from an orchestration point of view.

**This spec attempts to change the datapath semantics by reusing the VNIs** (unique for every VXLAN
enabled network in the cloud) **as dictated by the tenant's Openstack configuration for L2**
**switching and L3 forwarding**.

This implementation will completely remove the reliance on using the following within the DataCenter:

* Labels for L3 forwarding
* LPort tags for L2 switching

More specifically, the traffic from source VM will be routed in source OVS by the L3VPN / ELAN
pipeline. After that, the packet will travel as a switched packet in the VxLAN underlay within the
DC, containing the VNI in the VxLAN header instead of MPLS label / LPort Tag. In the destination
OVS, the packet will be collected and sent to the destination VM through the existing ELAN
pipeline.

In the nodes themselves, the LPortTag will continue to be used when pushing the packet from
ELAN / L3VPN pipeline towards the VM as ACLService continues to use LPortTags.

Simiarly ELANTags will continue to be used for handling broadcast packets:

* within the node for broadcasts generated within that node
* that land from another node to the current node via ``INTERNAL VxLAN Tunnels``

LPort tag uses 8 bits, ELAN tag uses 21 bits whereas VNI uses 24 bits in the metadata. Since these
are non-overlapping bits on the LSB side, no special handling is required while writing the metadata.

In Scope
--------
Since VNIs are provisioned only for VxLAN based underlays, which in turn are supported only for
traffic within the DataCenter, this feature has in its scope the use-cases pertaining to
**intra-DC connectivity over internal VxLAN tunnels only**.

Use Cases
---------
This feature involves amendments/testing pertaining to the following:

L3 use cases
++++++++++++

1. Router realization for subnet added as router-interface holding a pre-created VM.
2. Router realization for subnet added as router-interface when a new VM is booted on the subnet.
3. Router updated with an extra route to an existing VM.
4. Router updated to remove previously added one/more extra routes.
5. Retain SNAT functionality for external VLAN Provider Networks (transparent Internet VPN)
6. Retain SNAT functionality for external Flat Networks (transparent Internet VPN)
7. Retain SNAT functionality for tenant-orchestrated Internet VPN of type GRE (actually
   MPLSOverGRE)
8. Retain DNAT functionality for external VLAN Provider Networks (transparent Internet VPN)
9. Retain DNAT functionality for external Flat Networks (transparent Internet VPN)
10. Retain DNAT functionality for tenant-orchestrated Internet VPN of type GRE (actually
    MPLSOverGRE)


ELAN use cases
++++++++++++++

11. Unicast packet transmission within the hypervisor
12. Unicast packet transmission across hypervisors
13. Broadcast packet transmission within the hypervisor (local broadcast)
14. Broadcast packet transmission across hypervisors (remote broadcast)


Proposed change
===============

The following components within OpenDaylight Controller needs to be enhanced:

* VPN Engine (VPN Manager and VPN Interface Manager)
* FIB Manager
* VPN SubnetRoute Handler
* NAT Service
* ELAN Manager


Pipeline changes
----------------

L2 Switching
++++++++++++

Between VMs on a single OVS
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Unicast between VMs
~~~~~~~~~~~~~~~~~~~

There are no explicit pipeline changes for this use-case.

ARP Broadcast by the VM
~~~~~~~~~~~~~~~~~~~~~~~

Since the ARP broadcast by the VM will be a local broadcast on the VM's OVS itself, it will
continue to flood the packet to all VM ports by setting the LPortTag in the local broadcast
group. Hence, there are no explicit pipeline changes for this use-case.

The ARP response will be a unicast packet, and as indicated above, for unicast packets, there
are no explicit pipeline changes.

Between VMs on two different OVS
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Unicast between VMs
~~~~~~~~~~~~~~~~~~~

* `Ingress OVS`

  Instead of setting the LPortTag, VNI will be set in the ``tun_id`` field in
  ``L2_DMAC_FILTER_TABLE`` (table 51) while egressing the packet on the tunnel port.

  The modifications in flows and groups on the ingress OVS are illustrated below:

  .. code-block:: bash
     :emphasize-lines: 8

     cookie=0x8000000, duration=65.484s, table=0, n_packets=23, n_bytes=2016, priority=4,in_port=6actions=write_metadata:0x30000000000/0xffffff0000000001,goto_table:17
     cookie=0x6900000, duration=63.106s, table=17, n_packets=23, n_bytes=2016, priority=1,metadata=0x30000000000/0xffffff0000000000 actions=write_metadata:0x2000030000000000/0xfffffffffffffffe,goto_table:40
     cookie=0x6900000, duration=64.135s, table=40, n_packets=4, n_bytes=392, priority=61010,ip,dl_src=fa:16:3e:86:59:fd,nw_src=12.1.0.4 actions=ct(table=41,zone=5002)
     cookie=0x6900000, duration=5112.542s, table=41, n_packets=21, n_bytes=2058, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,17)
     cookie=0x8040000, duration=62.125s, table=17, n_packets=15, n_bytes=854, priority=6,metadata=0x6000030000000000/0xffffff0000000000 actions=write_metadata:0x700003138a000000/0xfffffffffffffffe,goto_table:48
     cookie=0x8500000, duration=5113.124s, table=48, n_packets=24, n_bytes=3044, priority=0 actions=resubmit(,49),resubmit(,50)
     cookie=0x805138a, duration=62.163s, table=50, n_packets=15, n_bytes=854, priority=20,metadata=0x3138a000000/0xfffffffff000000,dl_src=fa:16:3e:86:59:fd actions=goto_table:51
     cookie=0x803138a, duration=62.163s, table=51, n_packets=6, n_bytes=476, priority=20,metadata=0x138a000000/0xffff000000,dl_dst=fa:16:3e:31:fb:91 actions=set_field:**0x710**->tun_id,output:1


* `Egress OVS`

  On the egress OVS, for the packets coming in via the VxLAN tunnel, ``INTERNAL_TUNNEL_TABLE``
  currently matches on LPort tag for unicast packets. Since the incoming packets will now contain
  the VNI in the VxLAN header, the ``INTERNAL_TUNNEL_TABLE`` will match on the VNI, set the ELAN
  tag in the metadata and forward the packet to ``L2_DMAC_FILTER_TABLE`` so as to reach the
  destination VM via the ELAN pipeline.

  The modifications in flows and groups on the egress OVS are illustrated below:

  .. code-block:: bash
     :emphasize-lines: 2-7

     cookie=0x8000001, duration=5136.996s, table=0, n_packets=12601, n_bytes=899766, priority=5,in_port=1,actions=write_metadata:0x10000000001/0xfffff0000000001,goto_table:36
     cookie=0x9000004, duration=1145.594s, table=36, n_packets=15, n_bytes=476, priority=5,**tun_id=0x710,actions=write_metadata:0x138a000001/0xfffffffff000000,goto_table:51**
     cookie=0x803138a, duration=62.163s, table=51, n_packets=9, n_bytes=576, priority=20,metadata=0x138a000001/0xffff000000,dl_dst=fa:16:3e:86:59:fd actions=load:0x300->NXM_NX_REG6[],resubmit(,220)
     cookie=0x6900000, duration=63.122s, table=220, n_packets=9, n_bytes=1160, priority=6,reg6=0x300actions=load:0x70000300->NXM_NX_REG6[],write_metadata:0x7000030000000000/0xfffffffffffffffe,goto_table:251
     cookie=0x6900000, duration=65.479s, table=251, n_packets=8, n_bytes=392, priority=61010,ip,dl_dst=fa:16:3e:86:59:fd,nw_dst=12.1.0.4 actions=ct(table=252,zone=5002)
     cookie=0x6900000, duration=5112.299s, table=252, n_packets=19, n_bytes=1862, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,220)
     cookie=0x8000007, duration=63.123s, table=220, n_packets=8, n_bytes=1160, priority=7,reg6=0x70000300actions=output:6


ARP Broadcast by the VM on remote OVS
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* `Ingress OVS`

  Instead of setting the LPortTag, VNI will be set in the ``tun_id`` field in ``L2_DMAC_FILTER_TABLE``
  (table 51) while egressing the packet on the tunnel port.

  The modifications in flows and groups on the ingress OVS are illustrated below:

  .. code-block:: bash
     :emphasize-lines: 8

     cookie=0x8000000, duration=65.484s, table=0, n_packets=23, n_bytes=2016, priority=4,in_port=6actions=write_metadata:0x30000000000/0xffffff0000000001,goto_table:17
     cookie=0x6900000, duration=63.106s, table=17, n_packets=23, n_bytes=2016, priority=1,metadata=0x30000000000/0xffffff0000000000 actions=write_metadata:0x2000030000000000/0xfffffffffffffffe,goto_table:40
     cookie=0x6900000, duration=64.135s, table=40, n_packets=4, n_bytes=392, priority=61010,ip,dl_src=fa:16:3e:86:59:fd,nw_src=12.1.0.4 actions=ct(table=41,zone=5002)
     cookie=0x6900000, duration=5112.542s, table=41, n_packets=21, n_bytes=2058, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,17)
     cookie=0x8040000, duration=62.125s, table=17, n_packets=15, n_bytes=854, priority=6,metadata=0x6000030000000000/0xffffff0000000000 actions=write_metadata:0x700003138a000000/0xfffffffffffffffe,goto_table:48
     cookie=0x8500000, duration=5113.124s, table=48, n_packets=24, n_bytes=3044, priority=0 actions=resubmit(,49),resubmit(,50)
     cookie=0x805138a, duration=62.163s, table=50, n_packets=15, n_bytes=854, priority=20,metadata=0x3138a000000/0xfffffffff000000,dl_src=fa:16:3e:86:59:fd actions=goto_table:51
     cookie=0x803138a, duration=62.163s, table=51, n_packets=6, n_bytes=476, priority=20,metadata=0x138a000000/0xffff000000,dl_dst=fa:16:3e:31:fb:91 actions=set_field:**0x710**->tun_id,output:1


* `Egress OVS`

  On the egress OVS, for the packets coming in via the VxLAN tunnel, ``INTERNAL_TUNNEL_TABLE``
  currently matches on ELAN tag for broadcast packets. Since the incoming packets will now contain
  the VNI in the VxLAN header, the ``INTERNAL_TUNNEL_TABLE`` will match on the VNI, set the ELAN
  tag in the metadata and forward the packet to ``L2_DMAC_FILTER_TABLE`` to be broadcasted via the
  local broadcast groups traversing the ELAN pipeline.

  The modifications in flows and groups on the egress OVS are illustrated below:

  .. code-block:: bash
     :emphasize-lines: 2-12

     cookie=0x8000001, duration=5136.996s, table=0, n_packets=12601, n_bytes=899766, priority=5,in_port=1,actions=write_metadata:0x10000000001/0xfffff0000000001,goto_table:36
     cookie=0x9000004, duration=1145.594s, table=36, n_packets=15, n_bytes=476, priority=5,**tun_id=0x710,actions=write_metadata:0x138a000001/0xfffffffff000000,goto_table:51**
     cookie=0x8030000, duration=5137.609s, table=51, n_packets=9, n_bytes=1293, priority=0 actions=goto_table:52
     cookie=0x870138a, duration=1145.592s, table=52, n_packets=0, n_bytes=0, priority=5,metadata=0x138a000001/0xffff000001 actions=apply_actions(group:210003)

     group_id=210003,type=all,bucket=actions=set_field:0x4->tun_id,resubmit(,55)

     cookie=0x8800004, duration=1145.594s, table=55, n_packets=9, n_bytes=378, priority=9,tun_id=0x4,actions=load:0x400->NXM_NX_REG6[],resubmit(,220)
     cookie=0x6900000, duration=63.122s, table=220, n_packets=9, n_bytes=1160, priority=6,reg6=0x300actions=load:0x70000300->NXM_NX_REG6[],write_metadata:0x7000030000000000/0xfffffffffffffffe,goto_table:251
     cookie=0x6900000, duration=65.479s, table=251, n_packets=8, n_bytes=392, priority=61010,ip,dl_dst=fa:16:3e:86:59:fd,nw_dst=12.1.0.4 actions=ct(table=252,zone=5002)
     cookie=0x6900000, duration=5112.299s, table=252, n_packets=19, n_bytes=1862, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,220)
     cookie=0x8000007, duration=63.123s, table=220, n_packets=8, n_bytes=1160, priority=7,reg6=0x70000300actions=output:6


L3 Forwarding
+++++++++++++

Between VMs on a single OVS
^^^^^^^^^^^^^^^^^^^^^^^^^^^

There are no explicit pipeline changes for this use-case. However the pipeline a packet will
traverse through is shown below for understanding purposes.

.. code-block:: bash

   CLASSIFIER_TABLE => DISPATCHER_TABLE => INGRESS_ACL_TABLE => DISPATCHER_TABLE
   => L3_GW_MAC_TABLE => L3_FIB_TABLE => Nexthop Group (Set destination MAC
   address and LPortTag into NXMREG6) => EGRESS_DISPATCHER_TABLE =>
   EGRESS_ACL_TABLE => Output to destination VM port

The LPort tag will continue to be set in the nexthop group since when ``The EGRESS_DISPATCHER_TABLE``
sends the packet to ``EGRESS_ACL_TABLE``, it is used by the ACL service.

Between VMs on two different OVS
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

VM sourcing the traffic (Ingress OVS)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``L3_FIB_TABLE`` will set the destination L2 VNI in the ``tun_id`` field instead of the ``MPLS`` label.

.. code-block:: bash
   :emphasize-lines: 3

   CLASSIFIER_TABLE => DISPATCHER_TABLE => INGRESS_ACL_TABLE =>
   DISPATCHER_TABLE => L3_GW_MAC_TABLE =>
   L3_FIB_TABLE (set destination MAC, **set tunnel-ID as destination L2 VNI**)
   => Output to tunnel port

The modifications in flows and groups on the ingress OVS are illustrated below:

.. code-block:: bash
   :emphasize-lines: 11

   cookie=0x8000000, duration=128.140s, table=0, n_packets=25, n_bytes=2716, priority=4,in_port=5 actions=write_metadata:0x50000000000/0xffffff0000000001,goto_table:17
   cookie=0x8000000, duration=4876.599s, table=17, n_packets=0, n_bytes=0, priority=0,metadata=0x5000000000000000/0xf000000000000000 actions=write_metadata:0x6000000000000000/0xf000000000000000,goto_table:80
   cookie=0x1030000, duration=4876.563s, table=80, n_packets=0, n_bytes=0, priority=0 actions=resubmit(,17)
   cookie=0x6900000, duration=123.870s, table=17, n_packets=25, n_bytes=2716, priority=1,metadata=0x50000000000/0xffffff0000000000 actions=write_metadata:0x2000050000000000/0xfffffffffffffffe,goto_table:40
   cookie=0x6900000, duration=126.056s, table=40, n_packets=15, n_bytes=1470, priority=61010,ip,dl_src=fa:16:3e:63:ea:0c,nw_src=10.1.0.4 actions=ct(table=41,zone=5001)
   cookie=0x6900000, duration=4877.057s, table=41, n_packets=17, n_bytes=1666, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,17)
   cookie=0x6800001, duration=123.485s, table=17, n_packets=28, n_bytes=3584, priority=2,metadata=0x2000050000000000/0xffffff0000000000 actions=write_metadata:0x5000050000000000/0xfffffffffffffffe,goto_table:60
   cookie=0x6800000, duration=3566.900s, table=60, n_packets=24, n_bytes=2184, priority=0 actions=resubmit(,17)
   cookie=0x8000001, duration=123.456s, table=17, n_packets=17, n_bytes=1554, priority=5,metadata=0x5000050000000000/0xffffff0000000000 actions=write_metadata:0x60000500000222e0/0xfffffffffffffffe,goto_table:19
   cookie=0x8000009, duration=124.815s, table=19, n_packets=15, n_bytes=1470, priority=20,metadata=0x222e0/0xfffffffe,dl_dst=fa:16:3e:51:da:ee actions=goto_table:21
   cookie=0x8000003, duration=125.568s, table=21, n_packets=9, n_bytes=882, priority=42,ip,metadata=0x222e0/0xfffffffe,nw_dst=12.1.0.3 actions=**set_field:0x710->tun_id**,set_field:fa:16:3e:31:fb:91->eth_dst,output:1

VM receiving the traffic (Egress OVS)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On the egress OVS, for the packets coming in via the VxLAN tunnel, ``INTERNAL_TUNNEL_TABLE``
currently matches on ``MPLS label`` and sends it to the nexthop group to be taken to the destination
VM via ``EGRESS_ACL_TABLE``.
Since the incoming packets will now contain L2 VNI in the VxLAN header, the ``INTERNAL_TUNNEL_TABLE``
will match on the VNI, set the ELAN tag in the metadata and forward the packet to
``L2_DMAC_FILTER_TABLE``, from where it will be taken to the destination VM via the ELAN pipeline.

.. code-block:: bash
   :emphasize-lines: 1

   CLASSIFIER_TABLE => INTERNAL_TUNNEL_TABLE (Match on L2 VNI, set ELAN tag in the metadata)
   => L2_DMAC_FILTER_TABLE (Match on destination MAC) => EGRESS_DISPATCHER_TABLE
   => EGRESS_ACL_TABLE => Output to destination VM port

The modifications in flows and groups on the egress OVS are illustrated below:

.. code-block:: bash
   :emphasize-lines: 2-7

   cookie=0x8000001, duration=4918.647s, table=0, n_packets=12292, n_bytes=877616, priority=5,in_port=1actions=write_metadata:0x10000000001/0xfffff0000000001,goto_table:36
   cookie=0x9000004, duration=927.245s, table=36, n_packets=8234, n_bytes=52679, priority=5,**tun_id=0x710,actions=write_metadata:0x138a000001/0xfffffffff000000,goto_table:51**
   cookie=0x803138a, duration=62.163s, table=51, n_packets=9, n_bytes=576, priority=20,metadata=0x138a000001/0xffff000000,dl_dst=fa:16:3e:86:59:fd actions=load:0x300->NXM_NX_REG6[],resubmit(,220)
   cookie=0x6900000, duration=63.122s, table=220, n_packets=9, n_bytes=1160, priority=6,reg6=0x300actions=load:0x70000300->NXM_NX_REG6[],write_metadata:0x7000030000000000/0xfffffffffffffffe,goto_table:251
   cookie=0x6900000, duration=65.479s, table=251, n_packets=8, n_bytes=392, priority=61010,ip,dl_dst=fa:16:3e:86:59:fd,nw_dst=12.1.0.4 actions=ct(table=252,zone=5002)
   cookie=0x6900000, duration=5112.299s, table=252, n_packets=19, n_bytes=1862, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,220)
   cookie=0x8000007, duration=63.123s, table=220, n_packets=8, n_bytes=1160, priority=7,reg6=0x70000300actions=output:6


NAT Service
+++++++++++

DNAT
^^^^

For DNAT, we have three cases:

#. VLAN provider networks - not VNI based
#. Flat provider networks - not VNI based
#. MPLSoGRE provider networks - will continue to use labels

Hence, there are no explicit pipeline changes pertaining to DNAT.

SNAT
^^^^

From a VM on a NAPT switch to reach Internet, and reverse traffic reaching back to the VM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are no explicit pipeline changes for this use-case.

From a VM on a non-NAPT switch to reach Internet, and reverse traffic reaching back to the VM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In line with as discussed above for L3 forwarding between VMs on two different OVS, the
pipeline when a packet is transmitted from a non-NAPT switch to a NAPT switch, and back in the
reverse case will be similarly altered to use VNIs in the datapath. Also, the source MAC should be
set as the ``router_gateway_mac`` of the destination VM.

YANG changes
------------
None.


Configuration impact
--------------------
This change doesn't add or modify any configuration parameters.


Clustering considerations
-------------------------
No specific additional clustering considerations to be adhered to.


Other Infra considerations
--------------------------
None.


Security considerations
-----------------------
None.


Scale and Performance Impact
----------------------------
None.


Targeted Release(s)
-------------------
Carbon.

Known Limitations
-----------------
None.


Alternatives
------------
N.A.


Usage
=====

Features to Install
-------------------
odl-netvirt-openstack

REST API
--------
No new changes to the existing REST APIs.

CLI
---
No new CLI is being added.


Implementation
==============

Assignee(s)
-----------
Primary assignee:
  <Abhinav Gupta>
  <Vivekanandan Narasimhan>

Other contributors:
  <Kiran N Upadhyaya>


Work Items
----------

Trello card: https://trello.com/c/PfARbEmU/84-l3-forwarding-for-routers-using-lporttags

#. Code changes to alter the pipeline and e2e testing of the use-cases mentioned.
#. Add Documentation


Dependencies
============
This doesn't add any new dependencies.


Testing
=======

Unit Tests
----------
Appropriate UTs will be added for the new code coming in once framework is in place.

Integration Tests
-----------------
There won't be any Integration tests provided for this feature.

CSIT
----
No new testcases to be added, existing ones should continue to succeed.

Documentation Impact
====================
This will require changes to the Developer Guide.

Developer Guide needs to capture how this feature modifies the existing Netvirt L3 forwarding
service implementation.


References
==========

* http://docs.opendaylight.org/en/latest/documentation.html
* https://wiki.opendaylight.org/view/Genius:Carbon_Release_Plan
