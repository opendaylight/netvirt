.. contents:: Table of Contents
      :depth: 6

===============================================
VNI based L2 switching and L3 router forwarding
===============================================

https://git.opendaylight.org/gerrit/#/q/topic:vni-based-l2-l3

**Important: All gerrit links raised for this feature will have topic name as "vni-based-l2-l3"**

This feature realizes VNI (Virtual Network Identifier) based L2 switching and L3 router forwarding.
In doing so, it eliminates the usage of ``LPortTags``, ``ELAN tags`` and ``MPLS labels`` for
realizing the same on the wire and instead, makes use of VNIs supplied by the OpenStack using
tenant.

This will be selectively done for the use-cases covered by this spec and hence, its
implementation won't completely remove the usage of the above entities.

It is a step in the direction of enforcing datapath semantics that uses tenant supplied VNI values
on VXLAN Type networks created by tenants in Openstack Neutron.

**Note**: The existing L3 BGPVPN control-path and data-path semantics will continue to use L3
labels on the wire as well as inside the OVS datapaths of the hypervisor to realize both intra-dc
and inter-dc connectivity.


Problem description
===================

OpenDaylight NetVirt service today supports the following types of networks:

* Flat
* VLAN
* VxLAN
* GRE

Amongst these, VxLAN-based overlay is supported only for traffic within the DataCenter. External
network accesses over the DC-Gateway are supported via VLAN or GRE type external networks.
For rest of the traffic over the DC-Gateway, the only supported overlay is GRE.

Today, for VxLAN enabled networks by the tenant, the labels are generated by L3 forwarding service
and used. Such labels are re-used for inter-DC use-cases with BGPVPN as well. This does not honor
and is not in accordance with the datapath semantics from an orchestration point of view.

**This spec attempts to change the datapath semantics by enforcing the VNIs** (unique for every VXLAN
enabled network in the cloud) **as dictated by the tenant's Openstack configuration for L2**
**switching and L3 forwarding**.

This implementation will completely remove the reliance on using the following within the DataCenter:

* Labels for L3 forwarding
* LPort tags for L2 switching

More specifically, the traffic from source VM will be routed in source OVS by the L3VPN / ELAN
pipeline. After that, the packet will travel as a switched packet in the VxLAN underlay within the
DC, containing the VNI in the VxLAN header instead of MPLS label / LPort Tag. In the destination
OVS, the packet will be collected and sent to the destination VM through the existing ELAN
pipeline.

In the nodes themselves, the LPortTag will continue to be used when pushing the packet from
ELAN / L3VPN pipeline towards the VM as ACLService continues to use LPortTags.

Simiarly ELANTags will continue to be used for handling L2 broadcast packets:

* locally generated in the OVS datapath
* remotely received from another OVS datapath via ``INTERNAL VxLAN Tunnels``

LPort tag uses 8 bits and ELAN tag uses 21 bits in the metadata. The existing use of both in the
metadata will remain unaffected.

In Scope
--------
Since VNIs are provisioned only for VxLAN based underlays, this feature has in its scope the
use-cases pertaining to **intra-DC connectivity over internal VxLAN tunnels only**.

On the cloud data network wire, all the VxLAN traffic for basic L2 switching within a VxLAN
network and L3 forwarding across VxLAN-type networks using routers will use tenant supplied VNI
values for such VXLAN networks.

Inter-DC connectivity over external VxLAN tunnels is covered by the EVPN_RT5_ spec.

Out of Scope
------------

* *Complete removal of use of LportTags everywhere in ODL*: Use of LPortTags within the OVS Datapath
  of a hypervisor, for streaming traffic to the right virtual endpoint on that hypervisor (note:
  not on the wire) will be retained
* *Complete removal of use of ELANTags everywhere in ODL*: Use of ELANTags within the OVS Datapath
  to handle local/remote L2 broadcasts (note: not on the wire) will be retained
* *Complete removal of use of MPLS Labels everywhere in ODL*: Use of MPLS Labels for realizing an
  L3 BGPVPN (regardless of type of networks put into such BGPVPN that may include networks of type
  VXLAN) both on the wire and within the OVS Datapaths will be retained.

Complete removal of use of LPortTags, ELAN Tags and MPLS Labels for VXLAN-type networks has large
scale design/pipeline implications and thus need to be attempted as future initiatives via
respective specs.

Use Cases
---------
This feature involves amendments/testing pertaining to the following:

L3 forwarding use cases
+++++++++++++++++++++++

1. Router realized using VNIs for networks attached to a new router (with network having
   pre-created VMs)
2. Router realized using VNIs for networks attached to a new router (with new VMs booted later on
   the network)
3. Router updated with one or more extra route(s) to an existing VM.
4. Router updated to remove previously added one/more extra routes.
5. Retain SNAT functionality for external VLAN Provider Networks (transparent Internet VPN)
6. Retain SNAT functionality for external Flat Networks (transparent Internet VPN)
7. Retain SNAT functionality for tenant-orchestrated Internet VPN of type GRE (actually
   MPLSOverGRE)
8. Retain DNAT functionality for external VLAN Provider Networks (transparent Internet VPN)
9. Retain DNAT functionality for external Flat Networks (transparent Internet VPN)
10. Retain DNAT functionality for tenant-orchestrated Internet VPN of type GRE (actually
    MPLSOverGRE)


L2 switching use cases
++++++++++++++++++++++

11. L2 Unicast frames exchanged within an OVS Datapath
12. L2 Unicast frames exchanged over OVS Datapaths that are on different hypervisors
13. L2 Broadcast frames transmitted within an OVS Datapath
14. L2 Broadcast frames received from remote OVS Datapaths


Proposed change
===============

The following components within OpenDaylight Controller needs to be enhanced:

* NeutronVPN Manager
* VPN Engine (VPN Manager and VPN Interface Manager)
* FIB Manager
* VPN SubnetRoute Handler
* NAT Service
* ELAN Manager


Pipeline changes
----------------

L2 Switching
++++++++++++

Unicast
^^^^^^^

Within hypervisor
~~~~~~~~~~~~~~~~~

There are no explicit pipeline changes for this use-case.

Across hypervisors
~~~~~~~~~~~~~~~~~~

* `Ingress OVS`

  Instead of setting the destination LPortTag, destination network VNI will be set in the
  ``tun_id`` field in ``L2_DMAC_FILTER_TABLE`` (table 51) while egressing the packet on the tunnel
  port.

  The modifications in flows and groups on the ingress OVS are illustrated below:

  .. code-block:: bash
     :emphasize-lines: 8

     cookie=0x8000000, duration=65.484s, table=0, n_packets=23, n_bytes=2016, priority=4,in_port=6actions=write_metadata:0x30000000000/0xffffff0000000001,goto_table:17
     cookie=0x6900000, duration=63.106s, table=17, n_packets=23, n_bytes=2016, priority=1,metadata=0x30000000000/0xffffff0000000000 actions=write_metadata:0x2000030000000000/0xfffffffffffffffe,goto_table:40
     cookie=0x6900000, duration=64.135s, table=40, n_packets=4, n_bytes=392, priority=61010,ip,dl_src=fa:16:3e:86:59:fd,nw_src=12.1.0.4 actions=ct(table=41,zone=5002)
     cookie=0x6900000, duration=5112.542s, table=41, n_packets=21, n_bytes=2058, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,17)
     cookie=0x8040000, duration=62.125s, table=17, n_packets=15, n_bytes=854, priority=6,metadata=0x6000030000000000/0xffffff0000000000 actions=write_metadata:0x700003138a000000/0xfffffffffffffffe,goto_table:48
     cookie=0x8500000, duration=5113.124s, table=48, n_packets=24, n_bytes=3044, priority=0 actions=resubmit(,49),resubmit(,50)
     cookie=0x805138a, duration=62.163s, table=50, n_packets=15, n_bytes=854, priority=20,metadata=0x3138a000000/0xfffffffff000000,dl_src=fa:16:3e:86:59:fd actions=goto_table:51
     cookie=0x803138a, duration=62.163s, table=51, n_packets=6, n_bytes=476, priority=20,metadata=0x138a000000/0xffff000000,dl_dst=fa:16:3e:31:fb:91 actions=set_field:**0x710**->tun_id,output:1


* `Egress OVS`

  On the egress OVS, for the packets coming in via the internal VxLAN tunnel (OVS - OVS),
  ``INTERNAL_TUNNEL_TABLE`` currently matches on destination LPort tag for unicast packets. Since
  the incoming packets will now contain the network VNI in the VxLAN header, the
  ``INTERNAL_TUNNEL_TABLE`` will match on this VNI, set the ELAN tag in the metadata and forward
  the packet to ``L2_DMAC_FILTER_TABLE`` so as to reach the destination VM via the ELAN pipeline.

  The modifications in flows and groups on the egress OVS are illustrated below:

  .. code-block:: bash
     :emphasize-lines: 2-7

     cookie=0x8000001, duration=5136.996s, table=0, n_packets=12601, n_bytes=899766, priority=5,in_port=1,actions=write_metadata:0x10000000001/0xfffff0000000001,goto_table:36
     cookie=0x9000004, duration=1145.594s, table=36, n_packets=15, n_bytes=476, priority=5,**tun_id=0x710,actions=write_metadata:0x138a000001/0xfffffffff000000,goto_table:51**
     cookie=0x803138a, duration=62.163s, table=51, n_packets=9, n_bytes=576, priority=20,metadata=0x138a000001/0xffff000000,dl_dst=fa:16:3e:86:59:fd actions=load:0x300->NXM_NX_REG6[],resubmit(,220)
     cookie=0x6900000, duration=63.122s, table=220, n_packets=9, n_bytes=1160, priority=6,reg6=0x300actions=load:0x70000300->NXM_NX_REG6[],write_metadata:0x7000030000000000/0xfffffffffffffffe,goto_table:251
     cookie=0x6900000, duration=65.479s, table=251, n_packets=8, n_bytes=392, priority=61010,ip,dl_dst=fa:16:3e:86:59:fd,nw_dst=12.1.0.4 actions=ct(table=252,zone=5002)
     cookie=0x6900000, duration=5112.299s, table=252, n_packets=19, n_bytes=1862, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,220)
     cookie=0x8000007, duration=63.123s, table=220, n_packets=8, n_bytes=1160, priority=7,reg6=0x70000300actions=output:6


Broadcast
^^^^^^^^^

Across hypervisors
~~~~~~~~~~~~~~~~~~

The ARP broadcast by the VM will be a (local + remote) broadcast.

For the local broadcast on the VM's OVS itself, the packet will continue to get flooded to all the
VM ports by setting the destination LPortTag in the local broadcast group. Hence, there are no
explicit pipeline changes for when a packet is transmitted within the source OVS via a local
broadcast.

The changes in pipeline for the remote broadcast are illustrated below:


* `Ingress OVS`

  Instead of setting the ELAN Tag, network VNI will be set in the ``tun_id`` field as part of
  bucket actions in remote broadcast group while egressing the packet on the tunnel port.

  The modifications in flows and groups on the ingress OVS are illustrated below:

  .. code-block:: bash
     :emphasize-lines: 11

     cookie=0x8000000, duration=65.484s, table=0, n_packets=23, n_bytes=2016, priority=4,in_port=6actions=write_metadata:0x30000000000/0xffffff0000000001,goto_table:17
     cookie=0x6900000, duration=63.106s, table=17, n_packets=23, n_bytes=2016, priority=1,metadata=0x30000000000/0xffffff0000000000 actions=write_metadata:0x2000030000000000/0xfffffffffffffffe,goto_table:40
     cookie=0x6900000, duration=64.135s, table=40, n_packets=4, n_bytes=392, priority=61010,ip,dl_src=fa:16:3e:86:59:fd,nw_src=12.1.0.4 actions=ct(table=41,zone=5002)
     cookie=0x6900000, duration=5112.542s, table=41, n_packets=21, n_bytes=2058, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,17)
     cookie=0x8040000, duration=62.125s, table=17, n_packets=15, n_bytes=854, priority=6,metadata=0x6000030000000000/0xffffff0000000000 actions=write_metadata:0x700003138a000000/0xfffffffffffffffe,goto_table:48
     cookie=0x8500000, duration=5113.124s, table=48, n_packets=24, n_bytes=3044, priority=0 actions=resubmit(,49),resubmit(,50)
     cookie=0x805138a, duration=62.163s, table=50, n_packets=15, n_bytes=854, priority=20,metadata=0x3138a000000/0xfffffffff000000,dl_src=fa:16:3e:86:59:fd actions=goto_table:51
     cookie=0x8030000, duration=5112.911s, table=51, n_packets=18, n_bytes=2568, priority=0 actions=goto_table:52
     cookie=0x870138a, duration=62.163s, table=52, n_packets=9, n_bytes=378, priority=5,metadata=0x138a000000/0xffff000001 actions=write_actions(group:210004)

     group_id=210004,type=all,bucket=actions=group:210003,bucket=actions=set_field:**0x710**->tun_id,output:1


* `Egress OVS`

  On the egress OVS, for the packets coming in via the internal VxLAN tunnel (OVS - OVS),
  ``INTERNAL_TUNNEL_TABLE`` currently matches on ELAN tag for broadcast packets. Since the
  incoming packets will now contain the network VNI in the VxLAN header, the
  ``INTERNAL_TUNNEL_TABLE`` will match on this VNI, set the ELAN tag in the metadata and forward
  the packet to ``L2_DMAC_FILTER_TABLE`` to be broadcasted via the local broadcast groups
  traversing the ELAN pipeline.

  The ``TUNNEL_INGRESS_BIT`` being set in the ``CLASSIFIER_TABLE`` (table 0) ensures that the
  packet is always sent to the local broadcast group only and hence, remains within the OVS. This
  is necessary to avoid switching loop back to the source OVS.

  The modifications in flows and groups on the egress OVS are illustrated below:

  .. code-block:: bash
     :emphasize-lines: 2-12

     cookie=0x8000001, duration=5136.996s, table=0, n_packets=12601, n_bytes=899766, priority=5,in_port=1,actions=write_metadata:0x10000000001/0xfffff0000000001,goto_table:36
     cookie=0x9000004, duration=1145.594s, table=36, n_packets=15, n_bytes=476, priority=5,**tun_id=0x710,actions=write_metadata:0x138a000001/0xfffffffff000000,goto_table:51**
     cookie=0x8030000, duration=5137.609s, table=51, n_packets=9, n_bytes=1293, priority=0 actions=goto_table:52
     cookie=0x870138a, duration=1145.592s, table=52, n_packets=0, n_bytes=0, priority=5,metadata=0x138a000001/0xffff000001 actions=apply_actions(group:210003)

     group_id=210003,type=all,bucket=actions=set_field:0x4->tun_id,resubmit(,55)

     cookie=0x8800004, duration=1145.594s, table=55, n_packets=9, n_bytes=378, priority=9,tun_id=0x4,actions=load:0x400->NXM_NX_REG6[],resubmit(,220)
     cookie=0x6900000, duration=63.122s, table=220, n_packets=9, n_bytes=1160, priority=6,reg6=0x300actions=load:0x70000300->NXM_NX_REG6[],write_metadata:0x7000030000000000/0xfffffffffffffffe,goto_table:251
     cookie=0x6900000, duration=65.479s, table=251, n_packets=8, n_bytes=392, priority=61010,ip,dl_dst=fa:16:3e:86:59:fd,nw_dst=12.1.0.4 actions=ct(table=252,zone=5002)
     cookie=0x6900000, duration=5112.299s, table=252, n_packets=19, n_bytes=1862, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,220)
     cookie=0x8000007, duration=63.123s, table=220, n_packets=8, n_bytes=1160, priority=7,reg6=0x70000300actions=output:6


The ARP response will be a unicast packet, and as indicated above, for unicast packets, there
are no explicit pipeline changes.


L3 Forwarding
+++++++++++++

Between VMs on a single OVS
^^^^^^^^^^^^^^^^^^^^^^^^^^^

There are no explicit pipeline changes for this use-case.
The destination LPort tag will continue to be set in the nexthop group since when
``The EGRESS_DISPATCHER_TABLE`` sends the packet to ``EGRESS_ACL_TABLE``, it is used by the ACL
service.

Between VMs on two different OVS
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

L3 forwarding between VMs on two different hypervisors is asymmetric forwarding since the traffic
is routed in the source OVS Datapath while it is switched over the wire and then all the way to
the destination VM on the destination OVS Datapath.

VM sourcing the traffic (Ingress OVS)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``L3_FIB_TABLE`` will set the destination network VNI in the ``tun_id`` field instead of the
``MPLS`` label.

.. code-block:: bash
   :emphasize-lines: 3

   CLASSIFIER_TABLE => DISPATCHER_TABLE => INGRESS_ACL_TABLE =>
   DISPATCHER_TABLE => L3_GW_MAC_TABLE =>
   L3_FIB_TABLE (set destination MAC, **set tunnel-ID as destination network VNI**)
   => Output to tunnel port

The modifications in flows and groups on the ingress OVS are illustrated below:

.. code-block:: bash
   :emphasize-lines: 11

   cookie=0x8000000, duration=128.140s, table=0, n_packets=25, n_bytes=2716, priority=4,in_port=5 actions=write_metadata:0x50000000000/0xffffff0000000001,goto_table:17
   cookie=0x8000000, duration=4876.599s, table=17, n_packets=0, n_bytes=0, priority=0,metadata=0x5000000000000000/0xf000000000000000 actions=write_metadata:0x6000000000000000/0xf000000000000000,goto_table:80
   cookie=0x1030000, duration=4876.563s, table=80, n_packets=0, n_bytes=0, priority=0 actions=resubmit(,17)
   cookie=0x6900000, duration=123.870s, table=17, n_packets=25, n_bytes=2716, priority=1,metadata=0x50000000000/0xffffff0000000000 actions=write_metadata:0x2000050000000000/0xfffffffffffffffe,goto_table:40
   cookie=0x6900000, duration=126.056s, table=40, n_packets=15, n_bytes=1470, priority=61010,ip,dl_src=fa:16:3e:63:ea:0c,nw_src=10.1.0.4 actions=ct(table=41,zone=5001)
   cookie=0x6900000, duration=4877.057s, table=41, n_packets=17, n_bytes=1666, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,17)
   cookie=0x6800001, duration=123.485s, table=17, n_packets=28, n_bytes=3584, priority=2,metadata=0x2000050000000000/0xffffff0000000000 actions=write_metadata:0x5000050000000000/0xfffffffffffffffe,goto_table:60
   cookie=0x6800000, duration=3566.900s, table=60, n_packets=24, n_bytes=2184, priority=0 actions=resubmit(,17)
   cookie=0x8000001, duration=123.456s, table=17, n_packets=17, n_bytes=1554, priority=5,metadata=0x5000050000000000/0xffffff0000000000 actions=write_metadata:0x60000500000222e0/0xfffffffffffffffe,goto_table:19
   cookie=0x8000009, duration=124.815s, table=19, n_packets=15, n_bytes=1470, priority=20,metadata=0x222e0/0xfffffffe,dl_dst=fa:16:3e:51:da:ee actions=goto_table:21
   cookie=0x8000003, duration=125.568s, table=21, n_packets=9, n_bytes=882, priority=42,ip,metadata=0x222e0/0xfffffffe,nw_dst=12.1.0.3 actions=**set_field:0x710->tun_id**,set_field:fa:16:3e:31:fb:91->eth_dst,output:1

VM receiving the traffic (Egress OVS)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On the egress OVS, for the packets coming in via the VxLAN tunnel, ``INTERNAL_TUNNEL_TABLE``
currently matches on ``MPLS label`` and sends it to the nexthop group to be taken to the destination
VM via ``EGRESS_ACL_TABLE``.
Since the incoming packets will now contain network VNI in the VxLAN header, the ``INTERNAL_TUNNEL_TABLE``
will match on the VNI, set the ELAN tag in the metadata and forward the packet to
``L2_DMAC_FILTER_TABLE``, from where it will be taken to the destination VM via the ELAN pipeline.

.. code-block:: bash
   :emphasize-lines: 1

   CLASSIFIER_TABLE => INTERNAL_TUNNEL_TABLE (Match on network VNI, set ELAN tag in the metadata)
   => L2_DMAC_FILTER_TABLE (Match on destination MAC) => EGRESS_DISPATCHER_TABLE
   => EGRESS_ACL_TABLE => Output to destination VM port

The modifications in flows and groups on the egress OVS are illustrated below:

.. code-block:: bash
   :emphasize-lines: 2-7

   cookie=0x8000001, duration=4918.647s, table=0, n_packets=12292, n_bytes=877616, priority=5,in_port=1actions=write_metadata:0x10000000001/0xfffff0000000001,goto_table:36
   cookie=0x9000004, duration=927.245s, table=36, n_packets=8234, n_bytes=52679, priority=5,**tun_id=0x710,actions=write_metadata:0x138a000001/0xfffffffff000000,goto_table:51**
   cookie=0x803138a, duration=62.163s, table=51, n_packets=9, n_bytes=576, priority=20,metadata=0x138a000001/0xffff000000,dl_dst=fa:16:3e:86:59:fd actions=load:0x300->NXM_NX_REG6[],resubmit(,220)
   cookie=0x6900000, duration=63.122s, table=220, n_packets=9, n_bytes=1160, priority=6,reg6=0x300actions=load:0x70000300->NXM_NX_REG6[],write_metadata:0x7000030000000000/0xfffffffffffffffe,goto_table:251
   cookie=0x6900000, duration=65.479s, table=251, n_packets=8, n_bytes=392, priority=61010,ip,dl_dst=fa:16:3e:86:59:fd,nw_dst=12.1.0.4 actions=ct(table=252,zone=5002)
   cookie=0x6900000, duration=5112.299s, table=252, n_packets=19, n_bytes=1862, priority=62020,ct_state=-new+est-rel-inv+trk actions=resubmit(,220)
   cookie=0x8000007, duration=63.123s, table=220, n_packets=8, n_bytes=1160, priority=7,reg6=0x70000300actions=output:6


NAT Service
+++++++++++

DNAT
^^^^

For DNAT, we have cases for the following provider network types:

#. VLAN - not VNI based
#. Flat - not VNI based
#. VxLAN - VNI based (covered by the EVPN_RT5_ spec)
#. GRE - will continue to use MPLS labels

VNI from the pool to be used for VxLAN forwarding over MPLSoGRE.

SNAT
^^^^

From a VM on a NAPT switch to reach Internet, and reverse traffic reaching back to the VM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are no explicit pipeline changes for this use-case.

From a VM on a non-NAPT switch to reach Internet, and reverse traffic reaching back to the VM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In line with as discussed above for L3 forwarding between VMs on two different OVS, the
pipeline when a packet is transmitted from a non-NAPT switch to a NAPT switch, and back in the
reverse case will be similarly altered to use VNIs in the datapath.

Since there can be many networks with subnets added as router-interfaces, we do not have a unique
VNI to be sent out on the wire towards the designated NAPT switch doing the translation. Hence, an
``ODL VNI pool``, non-overlapping with OpenStack network segmentation ID pool range, will be
carved out to supply unique VNIs per router(Router VNI) and per Internet VPN(Internet VPN VNI).

On the non-NAPT switch, ``PSNAT_TABLE`` (table 26) will set the ``tun_id`` field with the Router VNI
allocated from the pool and send to group.

On the NAPT switch, ``INTERNAL_TUNNEL_TABLE`` (table 36) will match on the ``tun_id`` field which is
Router VNI and send the packet to ``OUTBOUND_NAPT_TABLE`` (table 46) to be taken to the Internet.

The modifications in flows and groups are illustrated below:

* `Non-NAPT switch`

  .. code-block:: bash
     :emphasize-lines: 1

     cookie=0x8000006, duration=2797.179s, table=26, n_packets=47, n_bytes=3196, priority=5,ip,
     metadata=0x23a50/0xfffffffe actions=**set_field:0x710->tun_id**,group:202501

     group_id=202501,type=all,bucket=actions=output:1

* `NAPT switch`

  .. code-block:: bash
     :emphasize-lines: 2

     cookie=0x8000001, duration=4918.647s, table=0, n_packets=12292, n_bytes=877616, priority=5,in_port=1actions=write_metadata:0x10000000001/0xfffff0000000001,goto_table:36
     cookie=0x9000004, duration=927.245s, table=36, n_packets=8234, n_bytes=52679, priority=10,ip,**tun_id=0x710**,actions=write_metadata:0x23a50/0xfffffffe,goto_table:46

SNAT to DNAT
^^^^^^^^^^^^

From a VM on a SNAT(NAPT) switch to reach Floating IP VM on another(DNAT) Switch
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

After SNAT Translation(VM IP to External Fixed IP), ``L3_FIB_TABLE`` (table=21) will be hit to reach
Floating IP VM on another Switch through vxlan tunnel. The Internet VPN VNI will be used as ``tun_id``.

* `SNAT Switch`

  .. code-block:: bash
     :emphasize-lines: 1

    cookie=0x8000003, duration=518.567s, table=21, n_packets=0, n_bytes=0, priority=42,ip, metadata=0x222e8/0xfffffffe,nw_dst=172.160.0.200 actions=**set_field:0x11178->tun_id**,output:9

On the DNAT Switch(having Floating IP VM), the ``INTERNAL_TUNNEL_TABLE(table 36)`` will match on the 
``tun_id`` field(Internet VPN VNI) and send the packet to ``PDNAT_TABLE`` (table 25) for DNAT forward 
Translation(Floating IP to VM IP)

* `DNAT Switch`

  .. code-block:: bash
     :emphasize-lines: 1

    cookie=0x9011179, duration=22.739s, table=36, n_packets=0, n_bytes=0, priority=5,**tun_id=0x11178** actions=resubmit(,25)

Response from Floating IP VM to reach SNAT switch having Non Floating IP which initiated traffic
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

After DNAT Reverse translation(VM IP to Floating IP), ``L3_FIB_TABLE`` (table=21) will be hit to reach
the External Fixed IP (assigned during SNAT translation). The Internet VPN VNI will be used as ``tun_id``.

* `DNAT Switch`
  .. code-block:: bash
     :emphasize-lines: 1

    cookie=0x8000003, duration=324.549s, table=21, n_packets=0, n_bytes=0, priority=42,ip,metadata=0x222e6/0xfffffffe,nw_dst=172.160.0.3 actions=**set_field:0x11174->tun_id**,output:2

On the SNAT Switch, the ``INTERNAL_TUNNEL_TABLE(table 36)`` will match on the ``tun_id`` field(Internet VPN VNI)
and send the packet to `` INBOUND_NAPT_TABLE`` (table 44) for SNAT reverse translation(External Fixed IP to VM IP)

* `SNAT Switch`

  .. code-block:: bash
     :emphasize-lines: 1

    cookie=0x9011174, duration=4059.289s, table=36, n_packets=0, n_bytes=0, priority=5,**tun_id=0x11174** actions=goto_table:44

DNAT to DNAT(Same Internet VPN)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Floating IP VM on DNAT Switch1 reaching another Floating IP VM on DNAT Switch2
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After DNAT reverse translation, the traffic will be sent on ``L3_FIB_TABLE`` (table=21) to reach the
Floating IP VM existing on DNAT Switch2. The Internet VPN VNI will be used as ``tun_id``.

* `DNAT Switch1`

  .. code-block:: bash
     :emphasize-lines: 1

    cookie=0x8000003, duration=518.567s, table=21, n_packets=0, n_bytes=0, priority=42,ip,metadata=0x222e8/0xfffffffe,nw_dst=172.160.0.200 actions=**set_field:0x11178->tun_id**,output:9

On reaching DNAT Switch2, the ``INTERNAL_TUNNEL_TABLE(table 36)`` will match on the ``tun_id``
field (Internet VPN VNI) and send the packet to ``PDNAT_TABLE`` (table 25) for DNAT forward
translation(Floating IP to VM IP).

* `DNAT Switch2`

  .. code-block:: bash
     :emphasize-lines: 1

    cookie=0x9011179, duration=22.739s, table=36, n_packets=0, n_bytes=0, priority=5,**tun_id=0x11178** actions=resubmit(,25)

As part of Response, the Internet VPN VNI will again be used as ``tun_id`` to reach Floating IP VM
on DNAT Switch1(same as above)

YANG changes
------------
None.


Configuration impact
--------------------
We have to make sure that we do not accept configuration of VxLAN type provider networks without
the ``segmentation-ID`` available in them since we are using it to represent the VNI on the wire
and in the flows/groups.


Clustering considerations
-------------------------
No specific additional clustering considerations to be adhered to.


Other Infra considerations
--------------------------
None.


Security considerations
-----------------------
None.


Scale and Performance Impact
----------------------------
None.


Targeted Release(s)
-------------------
Carbon.

Known Limitations
-----------------
None.


Alternatives
------------
N.A.


Usage
=====

Features to Install
-------------------
odl-netvirt-openstack

REST API
--------
No new changes to the existing REST APIs.

CLI
---
No new CLI is being added.


Implementation
==============

Assignee(s)
-----------
Primary assignee:
  <Abhinav Gupta>
  <Vivekanandan Narasimhan>

Other contributors:
  <Kiran N Upadhyaya>
  <Yugandhar Sarraju>
  <Chetan Arakere Gowdru>
  <Karthikeyan Krishnan>

Work Items
----------

Trello card: https://trello.com/c/9ZbSgfsj/96-enforce-vni-on-the-wire-for-both-l2-switching-and-l3-forwarding-on-vxlan-overlay-networks

#. Code changes to alter the pipeline and e2e testing of the use-cases mentioned.
#. Add Documentation


Dependencies
============
This doesn't add any new dependencies.


Testing
=======

Unit Tests
----------
Appropriate UTs will be added for the new code coming in once framework is in place.

Integration Tests
-----------------
There won't be any Integration tests provided for this feature.

CSIT
----
No new testcases to be added, existing ones should continue to succeed.

Documentation Impact
====================
This will require changes to the Developer Guide.

Developer Guide needs to capture how this feature modifies the existing Netvirt L3 forwarding
service implementation.


References
==========

* http://docs.opendaylight.org/en/latest/documentation.html
* https://wiki.opendaylight.org/view/Genius:Carbon_Release_Plan
* `EVPN_RT5 <https://tools.ietf.org/html/draft-ietf-bess-evpn-prefix-advertisement-03>`_
